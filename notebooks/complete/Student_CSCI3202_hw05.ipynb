{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Michael Brown\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "372b2e64a091bc5c1e72fcc5f697fe12",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# CSCI 3202, Spring 2021:  Assignment 5  \n",
    "\n",
    "Shortcuts:  [top](#top) -- [1](#p1) | [1a](#p1a) | [1b](#p1b) | [1c](#p1c) | [1d](#p1d) | [1e](#p1e) | [1f](#p1f) | [1g](#p1g) -- [2](#p2) | [2a](#p2a) | [2b](#p2b) | [2c](#p2c) | [2d](#p2d) | [2e](#p2e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c31566edfa1fdd0af96343452430d5f3",
     "grade": false,
     "grade_id": "overview",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment Overview\n",
    "\n",
    "This assignment is an exercise in implementing and analyzing Markov Decision Processes (MDPs). Problem 1 asks you to code a solution to a specific scenario, while Problem 2 is a conceptual question which asks you to describe an MDP problem of your own design.\n",
    "\n",
    "Here's a summary of the tasks required and the associated points:\n",
    "\n",
    "| Problem #  | Tasks                                                  | Points  |\n",
    "|:---        |:---                                                    |:---:    |\n",
    "| 1a         | Code: Complete implementation of `MDP` class           | 10      |\n",
    "| 1b         | Code: Implement `value_iteration` and `find_policy`    | 5       |\n",
    "| 1c         | Code and create: Generate and illustrate optimal path  | 5       |\n",
    "| 1d         | Written: analyze policy                                | 5       |\n",
    "| 1e         | Code: adjust non-terminal rewards                      | 5       |\n",
    "| 1f         | Code and write: adjust terminal rewards                | 5       |\n",
    "| 1g         | Written: analyze transition model                      | 5       |\n",
    "| 2a         | Written: define problem                                | 4       |\n",
    "| 2b         | Written: define states                                 | 4       |\n",
    "| 2c         | Written: define reward                                 | 4       |\n",
    "| 2d         | Written: define actions and transition                 | 4       |\n",
    "| 2e         | Written: define optimal policy                         | 4       |\n",
    "| Total      |                                                        | 60      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# add any imports you may need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c608e4fde84b1104aa33382b2642da7",
     "grade": false,
     "grade_id": "1-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "# Problem 1: Navigating an awkward situation with grace and poise\n",
    "\n",
    "<img src='https://www.explainxkcd.com/wiki/images/5/5f/interaction.png' style=\"width: 600px;\"/>\n",
    "\n",
    "Suppose you are at a social event where you would like to avoid any interaction with a large number of the other attendees. It's not that you don't like them, it's just that you don't like *talking to* them. A few of your good friends are also in attendance, but they are tucked away in a corner. The rectangular room in which the event is being held spans gridcells at $x=1,2,\\ldots, 6$ and $y=1,2,\\ldots, 5$. At the eastern edge ($x=6$) of this first floor room, there is a balcony, with a 6-foot drop. If the event becomes unbearably awkward, you can jump off the balcony and run away. Of course, this might hurt a little bit, so we should incorporate this into our reward structure.\n",
    "\n",
    "The terminal states and rewards associated with them are given in the diagram below. The states are represented as $(x,y)$ tuples. The available actions in non-terminal states include moving exactly 1 unit North (+y), South (-y), East (+x) or West (-x), although you should not include walking into walls, because that would be embarrassing in front of all these other people. Represent actions as one of 'N', 'S', 'E', or 'W'. For now, assume all non-terminal states have a default reward of -0.01, and use a discount factor of 0.99.\n",
    "\n",
    "<img src=\"http://www.cs.colorado.edu/~tonyewong/home/resources/hw06_mdp.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Use the following transition model for this decision process, if you are trying to move from state $s$ to state $s'$:\n",
    "* you successfully move from $s$ to $s'$ with probability 0.6\n",
    "* the remaining 0.4 probability is spread equally likely across state $s$ **and** all adjacent (N/S/E/W) states except for $s'$. Note that this does not necessarily mean that all adjacent states have 0.1, because some states do not have 4 adjacent states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "823e20cadbdae74fd72ea6227b26eb1c",
     "grade": false,
     "grade_id": "1a-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1a'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1a) - 10 points\n",
    "\n",
    "Complete the `MDP` class below. The docstring comments provide some desired specifications. You may add additional methods or attributes, if you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e28782999a0a5c35e0461daeb267be5c",
     "grade": false,
     "grade_id": "mdp-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, nrow, ncol, terminal_states, default_reward, df):\n",
    "        '''Create/store the following attributes:\n",
    "        self.states -- a list of all the states as (x,y) tuples\n",
    "        self.terminal_states -- a dictionary with terminal state keys, and rewards as values\n",
    "        self.default_reward -- the reward for being in any non-terminal state\n",
    "        self.df -- discount factor\n",
    "        ... and anything else you decide will be useful!\n",
    "        '''\n",
    "        self.states = []\n",
    "        for x in range(1, ncol+1):\n",
    "            for y in range(1, nrow+1):\n",
    "                self.states.append((x, y))\n",
    "\n",
    "        self.terminal_states = terminal_states\n",
    "        self.default_reward = default_reward\n",
    "        self.df = df\n",
    "        self.nrow = nrow\n",
    "        self.ncol = ncol\n",
    "        # self.terminal_states = { (6, y) : -5 for y in range(6) }\n",
    "        # people = {(x, y) : -1 for x, y in [(4,3), (4,4), (3,4)]}\n",
    "        # friends = {(x, y) : 2 for x, y in [(1,5), (1,4), (3,5), (1,3), (2,1), (3,1)]}\n",
    "        # self.terminal_states.update(people)\n",
    "        # terminal_states.update(friends)\n",
    "        # print(terminal_states.keys(), terminal_states.values())\n",
    "        # self.default_reward = -0.01\n",
    "        # self.df = 0.99\n",
    "\n",
    "\n",
    "    def actions(self, state):\n",
    "        \"\"\"\n",
    "        Return a list of available actions from the given state.\n",
    "        Possible actions are 'N','S','E','W'\n",
    "        [None] are the actions available from a terminal state.\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        x, y = state[0], state[1]\n",
    "\n",
    "        if state in self.terminal_states.keys():\n",
    "            return [None]\n",
    "        if not y == 1:\n",
    "            actions.append('S')\n",
    "        if not y == nrow:\n",
    "            actions.append('N')\n",
    "        if not x == 1:\n",
    "            actions.append('W')\n",
    "        if not x == ncol:\n",
    "            actions.append('E')\n",
    "        return actions\n",
    "\n",
    "\n",
    "    def reward(self, state):\n",
    "        '''Return the reward for being in the given state'''\n",
    "        if state in self.terminal_states:\n",
    "            return self.terminal_states[state]\n",
    "        else:\n",
    "            return self.default_reward\n",
    "\n",
    "    def result(self, state, action):\n",
    "        '''Return the resulting state (as a tuple) from doing the given\n",
    "        action in the given state, without uncertainty. Uncertainty\n",
    "        is incorporated into the transition method.\n",
    "        state -- a tuple representing the current state\n",
    "        action -- one of N, S, E or W, as a string\n",
    "        '''\n",
    "        \n",
    "        assert action in self.actions(state), 'Error: action needs to be available in that state'\n",
    "        assert state in self.states, 'Error: invalid state'\n",
    "        (x, y) = state\n",
    "        if action == 'N':\n",
    "            return x, y+1\n",
    "        elif action == 'S':\n",
    "            return x, y-1\n",
    "        elif action == 'W':\n",
    "            return x-1, y\n",
    "        elif action == 'E':\n",
    "            return x+1, y\n",
    "\n",
    "\n",
    "    def transition(self, state, action):\n",
    "        \"\"\"Return a list of (probability, next_state) associated\n",
    "        with the possibilities of taking the given action from the given state.\n",
    "        \"\"\"\n",
    "        \n",
    "        if action is None:\n",
    "            # This happens for a terminal state\n",
    "            return [(0, state)]\n",
    "        else:\n",
    "            # Not a terminal state\n",
    "            move_probs = []\n",
    "            # Pr of staying in same state\n",
    "            move_probs.append(\n",
    "                        (\n",
    "                            .4 / ( len(self.actions(state))),\n",
    "                            state\n",
    "                        )\n",
    "                    )\n",
    "            for accident in self.actions(state):\n",
    "                if accident != action:\n",
    "                    # Accidental move\n",
    "                    move_probs.append(\n",
    "                        (\n",
    "                            .4 / ( len(self.actions(state))),\n",
    "                            self.result(state, accident)\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    # Intended move\n",
    "                    move_probs.append(\n",
    "                        (\n",
    "                            .6,\n",
    "                            self.result(state, accident)\n",
    "                        )\n",
    "                    )\n",
    "            return move_probs\n",
    "        \n",
    "    def expected_utility(self, next_states, cur_util):\n",
    "        '''Return the expected utility given generated list of possible \n",
    "        next states and the current utility, which is a dictionary of the form {state : utility}\n",
    "        '''\n",
    "        sum_util = 0\n",
    "        for pr, state in next_states:\n",
    "            sum_util += cur_util[state] * pr\n",
    "        return sum_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75dc0b42cb21d9584de2037444b26d51",
     "grade": false,
     "grade_id": "1a-test-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (1a) tests\n",
    "\n",
    "Note that these are non-exhaustive, because there is some flexibility in how the `transition` method works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e36e4589b1131c2c413d7a804bf7f4c",
     "grade": true,
     "grade_id": "1a-test-simple",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed: 5 points\n"
     ]
    }
   ],
   "source": [
    "## BEGIN TESTS\n",
    "nrow = 3\n",
    "ncol = 3\n",
    "default_reward = -0.2\n",
    "discount = 0.5\n",
    "terminal = {(1,3):-1, (1,2):2}\n",
    "mdp_simple = MDP(nrow, ncol, terminal, default_reward, discount)\n",
    "\n",
    "actions1 = set(mdp_simple.actions((2,2)))\n",
    "assert (actions1 == {'N','S','E','W'}), \"Expected set of actions is {'N','S','E','W'}, your code returned: %s\" % actions1\n",
    "\n",
    "actions2 = set(mdp_simple.actions((1,1)))\n",
    "assert (actions2 == {'N','E'}), \"Expected set of actions is {'N','E'}, your code returned: %s\" % actions2\n",
    "\n",
    "actions3 = set(mdp_simple.actions((1,2)))\n",
    "assert (actions3 == {None}), \"Expected set of actions is {None}, your code returned: %s\" % actions3\n",
    "\n",
    "reward1 = mdp_simple.reward((1,2))\n",
    "assert (reward1 == 2), \"Expected reward is 2, your code returned: %f\" % reward1\n",
    "\n",
    "reward2 = mdp_simple.reward((2,2))\n",
    "assert (reward2 == -0.2), \"Expected reward is -0.2, your code returned: %f\" % reward2\n",
    "\n",
    "result1 = mdp_simple.result((1,1), 'N')\n",
    "assert (result1 == (1,2)), \"Expected result is (1,2), your code returned: %s\" % (result1,)\n",
    "\n",
    "print(\"All tests passed: 5 points\")\n",
    "## END TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe6b6124959228ec56ece931028694a1",
     "grade": true,
     "grade_id": "1a-test-problem",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed: 5 points\n"
     ]
    }
   ],
   "source": [
    "## BEGIN TESTS\n",
    "# set values from problem statement\n",
    "nrow = 5\n",
    "ncol = 6\n",
    "default_reward = -0.01\n",
    "discount = 0.99\n",
    "terminal = {(1,3):-1, (1,4):2, (1,5):2, (2,1):-1, (3,1):-1, (3,4):-1, (3,5):1,\n",
    "            (4,3):-1, (4,4):-1, (6,1):-5, (6,2):-5, (6,3):-5, (6,4):-5, (6,5):-5}\n",
    "mdp_p1 = MDP(nrow, ncol, terminal, default_reward, discount)\n",
    "\n",
    "# Find the expected utility of walking N from (1,1):\n",
    "util_old = {s : s[0]+s[1] for s in mdp_p1.states}\n",
    "\n",
    "next_states = mdp_p1.transition((1,1), 'N')\n",
    "assert (len(next_states) == 3), \"Expected 3 possible next states, your code returned: %d\" % len(next_states)\n",
    "\n",
    "exp_util = mdp_p1.expected_utility(next_states, util_old)\n",
    "assert (exp_util == 2.8), \"Expected utility should be 2.8, your code returned %f\" % exp_util\n",
    "\n",
    "print(\"All tests passed: 5 points\")\n",
    "## END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1171844cdb0baf1260559a6f58a18477",
     "grade": false,
     "grade_id": "1b-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1b'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1b) - 5 points\n",
    "\n",
    "Implement value iteration to calculate the utilities for each state.  Also implement a function that takes as arguments an `MDP` object and a dictionary of state-utility pairs (key-value) and returns a dictionary for the optimal policy.  The optimal policy dictionary should have state tuples as keys and the optimal move (None, N, S, E or W) as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85c8c83a0cd970ca44ba24d8f8715781",
     "grade": false,
     "grade_id": "value-iter-find-policy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, tol=1e-3):\n",
    "    # TODO: \n",
    "    # 1. initialize utility for all states to 0\n",
    "    # 2. for each state on the board\n",
    "    #    2.1. calculate expected utility for each possible next state\n",
    "    #    2.2. find best utility out of possible expected utilities\n",
    "    #    2.3. define new utility of the state\n",
    "    # 3. Repeat 2 until problem is converged\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    util = {state:0 for state in mdp.states}\n",
    "    while True:\n",
    "        temp_utils = util.copy()\n",
    "        max_change = 0.0\n",
    "        for state in mdp.states:\n",
    "            possible_actions = mdp.actions(state)\n",
    "            next_states = [mdp.transition(state,act) for act in possible_actions]\n",
    "            best_util = -1000.0\n",
    "\n",
    "            for i in range(len(next_states)):\n",
    "                exp_util = mdp.expected_utility(next_states[i], temp_utils)\n",
    "                best_util = max(best_util,exp_util)\n",
    "\n",
    "            util[state] = mdp.reward(state) + mdp.df * best_util\n",
    "            max_change = max(max_change,abs(util[state] - temp_utils[state]))\n",
    "        if max_change < tol:\n",
    "            break\n",
    "    return util\n",
    "\n",
    "def find_policy(mdp, utility):\n",
    "    '''Return a dictionary containing the best policy for each state s,\n",
    "    of the form {s : s_policy}\n",
    "    '''\n",
    "    results = {}\n",
    "    for state in mdp.states:\n",
    "        best_action = None\n",
    "        best_util = -1000\n",
    "        for act in mdp.actions(state):\n",
    "            cur_util = utility[state]\n",
    "            exp_util = mdp.expected_utility(mdp.transition(state, act), utility)\n",
    "            if exp_util > best_util:\n",
    "                best_util = exp_util\n",
    "                best_action = act\n",
    "        results[state] = best_action\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f14e15e8c0ad815923f021a8797e81e",
     "grade": false,
     "grade_id": "1b-tests-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (1b) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54d20c4e3b584c7e7863d35660db1f6c",
     "grade": true,
     "grade_id": "1b-tests-asserts",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test cases: 5 points\n"
     ]
    }
   ],
   "source": [
    "## BEGIN TESTS\n",
    "utility = value_iteration(mdp_p1, tol=1e-6)\n",
    "policy = find_policy(mdp_p1, utility)\n",
    "\n",
    "util1 = utility[(1,5)]\n",
    "assert (util1 == 2), \"Expected utility of 2, your code returned %f\" % util1\n",
    "\n",
    "util2 = utility[(6,1)]\n",
    "assert (util2 == -5), \"Expected utility of -5, your code returned %f\" % util2\n",
    "\n",
    "util3 = round(utility[(2,5)],2)\n",
    "assert (util3 == 1.74), \"Expected utility of 1.74, your code returned %f\" % util3\n",
    "\n",
    "util4 = round(utility[(5,3)],2)\n",
    "assert (util4 == -1.39), \"Expected utility of -1.39, your code returned %f\" % util4\n",
    "\n",
    "policy1 = policy[(2,4)]\n",
    "assert (policy1 == 'W'), \"Expected policy is 'W', your code returned %s\" % policy1\n",
    "\n",
    "policy2 = policy[(1,1)]\n",
    "assert (policy2 == 'N'), \"Expected policy is 'N', your code returned %s\" % policy2\n",
    "\n",
    "print(\"Passed test cases: 5 points\")\n",
    "## END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1b6b83bb256c438e69fc7bb280e5a80",
     "grade": false,
     "grade_id": "1c-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1c'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1c) - 5 points\n",
    "\n",
    "If we enter the room at $s_0$, what is the optimal path for us to follow? Complete the following function to generate the sequence of states along the path. If we start in state $s_0$, then your output should be in the form $[s_0, s_1, s_2, ... , s_{term}]$ where $s_{term}$ is a terminal state. Set your tolerance for value iteration to be $10^{-6}$\n",
    "\n",
    "Additionally, create a graphic to illustrate this policy pathway, either by generating a plot in Python or by uploading a hand-drawn image and including a link to it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "740f51d6586bfb8d597b3afe8e46b108",
     "grade": false,
     "grade_id": "1c-answer-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_optimal_path(mdp, state):\n",
    "    '''Generate list of states visited along the optimal path given an MDP\n",
    "    instance and the starting state\n",
    "    '''\n",
    "\n",
    "    visited = []\n",
    "    while True:\n",
    "        visited.append(state)\n",
    "        utility = value_iteration(mdp, tol=1e-6)\n",
    "        policy = find_policy(mdp, utility)\n",
    "        state = mdp.result(state, policy[state])\n",
    "        if state in mdp.terminal_states:\n",
    "            visited.append(state)\n",
    "            return visited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "085e7a8e272b36a76e8ce82e1537152e",
     "grade": false,
     "grade_id": "1c-answer-graphic",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Put a link to your graphic below for the path from (5,3). You can find an [example image here](http://www.cs.colorado.edu/~tonyewong/home/resources/hw06_mdp_path.png) with the optimal path starting from (5,1). **Please include a link rather than attaching a file**. This can be a link to your file in Google Drive, with the permissions set to public. The graders will not ask for permissions! The syntax for including a link in markdown is `[link text](url.com/to/your/image)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bd08159d2a99ee23492b15403f5e025",
     "grade": true,
     "grade_id": "1c-graphic",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path1 = find_optimal_path(mdp_p1, (5,3))\n",
    "assert (path1 == [(5, 3), (5, 2), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)]), \"The optimal path is [(5, 3), (5, 2), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)], your code generated %s\" % path1\n",
    "x = path[0]\n",
    "y = path[1]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1c) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f5c9f5250a8cb8db949d1166219cf06",
     "grade": true,
     "grade_id": "1c-tests-asserts",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed tests: 3 points\n"
     ]
    }
   ],
   "source": [
    "## BEGIN TESTS\n",
    "path1 = find_optimal_path(mdp_p1, (5,3))\n",
    "assert (path1 == [(5, 3), (5, 2), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)]), \"The optimal path is [(5, 3), (5, 2), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)], your code generated %s\" % path1\n",
    "\n",
    "path2 = find_optimal_path(mdp_p1, (5,1))\n",
    "assert (path2 == [(5, 1), (4, 1), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)]), \"The optimal path is [(5, 1), (4, 1), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)], your code generated %s\" % path2\n",
    "\n",
    "path3 = find_optimal_path(mdp_p1, (1,1))\n",
    "assert (path3 == [(1, 1), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4)]), \"The optimal path is [(1, 1), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4)], your code generated %s\" % path3\n",
    "\n",
    "print(\"Passed tests: 3 points\")\n",
    "## END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b468abfa8603618ab21ea57d3a26d564",
     "grade": false,
     "grade_id": "1d-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1d'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1d) - 5 points\n",
    "\n",
    "From (3,2) the optimal move is to walk West. If we are trying to go talk to our friends in the Northwest corner, why would we rather do this than walk North first, then West?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75467f69d87838440cb5b2a3dd342a34",
     "grade": true,
     "grade_id": "1d-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The cell to the west at (2,2) has a lower utility than the square to the north at (3,3).\n",
    "\n",
    "This is because cell (2,2) has less neighboring terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "745fc8b0c5373d58b894072f14d2bcf8",
     "grade": false,
     "grade_id": "1e-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1e'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1e) - 5 points\n",
    "\n",
    "How painfully awkward do you need to set the default reward for non-terminal states before the optimal move at (5,1) becomes jumping off the balcony immediately and running away? Implement the following function which returns the reward where the policy for (5,1) is to jump off the balcony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83a84df7c9004254b67cada546239f68",
     "grade": false,
     "grade_id": "1e-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_non_terminal_reward():\n",
    "    nrow = 5\n",
    "    ncol = 6\n",
    "    discount = 0.99\n",
    "\n",
    "    terminal = {(1,3):-1, (1,4):2, (1,5):2, (2,1):-1, (3,1):-1, (3,4):-1, (3,5):1,\n",
    "                (4,3):-1, (4,4):-1, (6,1):-5, (6,2):-5, (6,3):-5, (6,4):-5, (6,5):-5}\n",
    "    for reward in np.arange(-0.01, -3, -0.01):\n",
    "        mdp = MDP(nrow, ncol, terminal, reward, discount)\n",
    "        utility = value_iteration(mdp, tol=1e-6)\n",
    "        policy = find_policy(mdp, utility)\n",
    "        if policy[(5,1)] == 'E':\n",
    "            return reward\n",
    "        #\n",
    "        # if state in mdp.terminal_states:\n",
    "        # if len(find_optimal_path(MDP(nrow, ncol, terminal, reward, discount), (5,1)) ) == 1:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1e) tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4346aaa5a01e06b338b3482b225aaa30",
     "grade": true,
     "grade_id": "1e-tests-assert",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed: 5 points\n"
     ]
    }
   ],
   "source": [
    "reward1 = find_non_terminal_reward()\n",
    "\n",
    "assert (reward1 == -2.09), \"The expected reward is -2.09, your code returned %f\" % reward1\n",
    "\n",
    "print(\"Test passed: 5 points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e194e2f330afbd4fe1437648f6f86723",
     "grade": false,
     "grade_id": "1f-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1f'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1f) - 5 points\n",
    "\n",
    "In **1e** we assumed a certain level of loss (negative reward) just for being present.  But a more realistic approach might be to instead change the reward structure for the terminal states. Consider the terminal states with -1 reward in the default model. Let $R^*$ denote the reward associated with these states. How low does $R^*$ need to be in order for us to immediately jump off the balcony and run away? Use the default non-terminal state reward of -0.01. Implement the following function to return the value of $R^*$ which leads to a policy of jumping off the balcony at (5,1). Write a few sentences interpreting your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03cd66008c7b709512677ede48059600",
     "grade": false,
     "grade_id": "1f-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_terminal_reward():\n",
    "    nrow = 5\n",
    "    ncol = 6\n",
    "    discount = 0.99\n",
    "    reward = -0.01\n",
    "    for rstar in np.arange(-6, -12, -0.01):\n",
    "        # TODO:\n",
    "        # 1. set the reward of the terminal nodes \n",
    "        # 2. define MDP with appropriate parameters\n",
    "        # 3. find policy for state (5,1)\n",
    "        # 4. return R* if the policy for (5,1) is to jump off the balcony\n",
    "\n",
    "        terminal = {(1,3):rstar, (1,4):2, (1,5):2, (2,1):rstar, (3,1):rstar, (3,4):rstar, (3,5):1,\n",
    "                    (4,3):rstar, (4,4):rstar, (6,1):-5, (6,2):-5, (6,3):-5, (6,4):-5, (6,5):-5}\n",
    "\n",
    "        mdp_p1 = MDP(nrow, ncol, terminal, reward, discount)\n",
    "        utility = value_iteration(mdp_p1, tol=1e-6)\n",
    "        policy = find_policy(mdp_p1, utility)\n",
    "        if policy[(5,1)] == 'E':\n",
    "            return rstar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7008fb02d7df7ba16c7882585e9b890d",
     "grade": false,
     "grade_id": "1f-test-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (1f) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5230fc87068ca9643b6d7333441b715",
     "grade": true,
     "grade_id": "1f-test-asserts",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test: 3 points\n"
     ]
    }
   ],
   "source": [
    "reward1 = round(find_terminal_reward(),2)\n",
    "assert (reward1 == -11.39), \"Expected reward is -11.39, your code returned %f\" % reward1\n",
    "\n",
    "print(\"Passed test: 3 points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b7fd68f561b33ad94ed8a286a7eb3d6",
     "grade": false,
     "grade_id": "1f-reflection-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write a few sentences with your interpretation here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12e68d9c45421d6fd097011fd0b9f8bd",
     "grade": true,
     "grade_id": "1f-answer",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Our range went started at -6 and decreased to -12 at decrements of 0.01.\n",
    "\n",
    "This means that when the reward of running into people is set to -11.39,\n",
    "the risk of running into them is so high that you would rather jump off the the balconey and run away.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d2e45eceb59f1d36e0c528cd7faf22b",
     "grade": false,
     "grade_id": "1g-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1g'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1g) - 5 points\n",
    "\n",
    "Given the problem context, write a few sentences about why this is or is not an appropriate transition model. Include an interpretation of the terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a884715306f6da7b8d2b7fd566f7732e",
     "grade": true,
     "grade_id": "1g-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "This is not an appropriate model because its not that bad to run into people (in my opinion). However, contracting COVID 18 can  result\n",
    "in a terminal state. One might assign a very negative reward with the possibility of running into someone they know does\n",
    "not have covid vaccine. Even with stranger danger, jumping off a balcony is a little much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5038f4053461860ec0728cc4626f059b",
     "grade": false,
     "grade_id": "2-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "<a id='p2'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "# Problem 2: Define your very own MDP\n",
    "\n",
    "For this problem, you do not need to write any code, but rather communicate your ideas clearly using complete sentences and descriptions of the concepts the questions ask about. You can, of course, include some pseudocode if it helps, but that is not strictly necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e12e0af4a88a181762bd42ae609e93cf",
     "grade": false,
     "grade_id": "2a-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2a'></a>\n",
    "\n",
    "## (2a) - 4 points\n",
    "\n",
    "Describe something you think would be interesting to model using a Markov decision process.  Be **creative** - do not use any examples from your homework, class, or the textbook, and if you are working with other students, please **come up with your own example**. There are so, SO many possible answers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d13a848d94d3a2552c856f613b1fd2d",
     "grade": true,
     "grade_id": "2a-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "I think it would be interesting to model Scientific Paper Relevance with MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33940776445caeaffd19100073cef2d0",
     "grade": false,
     "grade_id": "2b-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2b'></a>\n",
    "\n",
    "## (2b) - 4 points\n",
    "\n",
    "What are the states associated with your MDP? Include a discussion of terminal/non-terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c536e75b992639fa2b21877a388eda0f",
     "grade": true,
     "grade_id": "2b-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Initial State:\n",
    "- A feed of research papers ordered by publication date.\n",
    "\n",
    "An ordering of 10 papers in a terminal state.\n",
    "\n",
    "Each state is an ordering of the papers, or a subset of some ordering of the papers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05b9b36293b64ab2d3ba757666ac3427",
     "grade": false,
     "grade_id": "2c-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2c'></a>\n",
    "\n",
    "## (2c) - 4 points\n",
    "\n",
    "What is the reward structure associated with your MDP?  Include a discussion of terminal/non-terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90b36dc1c0b6d151da8f42c1d93b9d9a",
     "grade": true,
     "grade_id": "2c-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "First, we would assign relevance to a paper with factors such as: \n",
    "\n",
    "- Reputation of publisher\n",
    "\n",
    "- Number of hot papers a researcher has published\n",
    "\n",
    "- The publishers interests compared to the viewers\n",
    "\n",
    "- Rate of new citations for a the paper at question, and the rate of citations for similar papers\n",
    "\n",
    "- Similarity of the paper with other papers the reader likes\n",
    "\n",
    "\n",
    "The order in which the papers are presented determines how much reward is assosiated with the state. The more relevant papers near the beginning, the higher the reward. Some papers might have have negative relevance. In that case they would bring down the reward of the state. Also, too many similar papers might have negative reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "565e36ab6960052d86fa2a08c18f96c4",
     "grade": false,
     "grade_id": "2d-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2d'></a>\n",
    "\n",
    "## (2d) - 4 points\n",
    "\n",
    "What are the actions and transition model associated with your MDP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b16a36bb225d610a1837822e543162f1",
     "grade": true,
     "grade_id": "2d-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The mdp can swap the posistion of any two papers in order to move to the next state. \n",
    "\n",
    "Also, the MDP can remove a paper from the set.\n",
    "\n",
    "The MDP can not remove all papers from a set.\n",
    "\n",
    "The longer it takes to build a good feed, the more compute time we have to pay. Thus, there should be a small negative reward for non-terminal states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35ab2fc9b4eb20ce1b32e1a2aa0e1007",
     "grade": false,
     "grade_id": "2e-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2e'></a>\n",
    "\n",
    "## (2e) - 4 points\n",
    "\n",
    "Interpret what an optimal policy represents in the context of your particular MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c57bad2508d2aa53541aaaa24b5f55c3",
     "grade": true,
     "grade_id": "2e-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Ideally we would want the papers ordered specifically to present the most relevant one first, second most next and so on. We do not want to present papers that are irrelevant to a researcher. At the same time, we don't want to present too many papers covering the same idea, as repitition is boring. \n",
    "\n",
    "Remove papers that are extremely irrelevant to save computing time spent on searching for swaps. Swap papers to build an optimal ordering. Remove papers that are extremely irrelevant. Terminate\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "519px",
    "left": "22px",
    "top": "149px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
